// npm run start:test -- --start-app --exclude:postgres --region-url:http://localhost:4000/regioncategories --postgres:host.docker.internal:32769:testuser:testpassword:testdb
import {
  StartedPostgreSqlContainer,
  PostgreSqlContainer,
} from '@testcontainers/postgresql';
// import { KafkaContainer } from '@testcontainers/kafka';
import {
  GenericContainer,
  Network,
  StartedNetwork,
  StartedTestContainer,
  Wait,
} from 'testcontainers';
import { ElasticsearchContainer } from '@testcontainers/elasticsearch';
import { getCommandParameters } from './utils/get-command-parapmeter';
import { ChildProcess, spawn } from 'child_process';
import path from 'path';

let postgres: StartedPostgreSqlContainer | undefined;
let kafka: StartedTestContainer | undefined;
let kafkaUrl: string | undefined;
let kafkaInternalBootstrapServer: string | undefined;
let connect: StartedTestContainer | undefined;
let elastic: StartedTestContainer | undefined;
let elasticUrl: string | undefined;
let internalElasticsearchUrl: string | undefined;
let kafkaUi: StartedTestContainer | undefined;
let kafkaUiUrl: string | undefined;
let kibana: StartedTestContainer | undefined;
let kibanaUrl: string | undefined;
let ksqlDb: StartedTestContainer | undefined;
let nestProcess: ChildProcess | undefined;

let network: StartedNetwork | undefined;

const KAFKA_HOST_NAME = 'mecipe-test-kafka';
const ELASTICSEARCH_HOST_NAME = 'mecipe-test-elasticsearch';
const POSTGRES_HOST_NAME = 'mecipe-test-postgres';
const DEBEZIUM_HOST_NAME = 'mecipe-test-debezium';
const KAFKA_UI_HOST_NAME = 'mecipe-test-kafka-ui';
const KIBANA_HOST_NAME = 'mecipe-test-kibana';
const EXTERNAL_HOST_NAME = 'host.docker.internal'; // Docker Desktop í™˜ê²½
const ELASTIC_USERNAME = 'elastic';
const ELASTIC_PASSWORD = 'elasticpassword';
const KSQLDB_HOST_NAME = 'mecipe-test-ksqldb';

// DEBEZIUM_POSTGRES_CONNECTOR_CONFIG í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ host.docker.internalì„ ì‚¬ìš©
// Testcontainersê°€ ì™¸ë¶€ë¡œ ë…¸ì¶œëœ Postgresë¥¼ ì§ì ‘ ì œì–´í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
// PostgresHostëŠ” Testcontainers ì™¸ë¶€ì˜ í˜¸ìŠ¤íŠ¸ ì£¼ì†Œë¥¼ ì „ë‹¬í•´ì•¼ í•¨.
const DEBEZIUM_POSTGRES_CONNECTOR_CONFIG_FOR_EXTERNAL_DB = (
  dbHost: string = EXTERNAL_HOST_NAME, // ì—¬ê¸°ì„œëŠ” 'host.docker.internal'
  dbPort: string | number, // ì—¬ê¸°ì„œëŠ” ì˜ˆ:'32769'
  dbName: string,
  dbUser: string,
  dbPass: string,
) =>
  JSON.stringify({
    name: 'cafe-infos-debezium-connector',
    config: {
      'connector.class': 'io.debezium.connector.postgresql.PostgresConnector',
      'plugin.name': 'pgoutput',
      'tasks.max': '1',
      'database.hostname': dbHost, // âœ¨ host.docker.internal ì‚¬ìš©!
      'database.port': dbPort, // âœ¨ ì™¸ë¶€ ë…¸ì¶œëœ í¬íŠ¸ ì‚¬ìš©!
      'database.user': dbUser,
      'database.password': dbPass,
      'database.dbname': dbName,
      'database.server.name': 'dbserver',
      'topic.prefix': 'dbserver',
      'table.include.list': 'public.CafeInfo,public.RegionCategory',
      'publication.autocreate.mode': 'all_tables',
      'slot.name': 'debezium_slot',
      'heartbeat.interval.ms': '5000',
      'value.converter': 'org.apache.kafka.connect.json.JsonConverter',
      'value.converter.schemas.enable': 'false',
      'key.converter': 'org.apache.kafka.connect.json.JsonConverter',
      'key.converter.schemas.enable': 'false',
    },
  });

async function startPostgres(network: StartedNetwork) {
  console.log('ğŸ”„ Starting Postgres...');
  postgres = await new PostgreSqlContainer('debezium/postgres:16-alpine')
    .withNetwork(network)
    .withNetworkAliases(POSTGRES_HOST_NAME)
    .withEnvironment({
      POSTGRES_USER: 'postgres',
      POSTGRES_PASSWORD: 'postgres',
      // PostgreSQL ì„¤ì • ë°˜ì˜S
      POSTGRES_DB: 'mydb',
    })
    .start();
  console.log(
    'âœ… Postgres started',
    `host: ${postgres?.getHost()}`,
    `port: ${postgres?.getMappedPort(5432).toString()}`,
  );
}

async function startKafka(network: StartedNetwork) {
  console.log('ğŸ”„ Starting Kafka...');
  kafkaInternalBootstrapServer = `${KAFKA_HOST_NAME}:29092`;

  // ê³ ì • í¬íŠ¸ ì‚¬ìš©: ë™ì  í¬íŠ¸ í• ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
  // í˜¸ìŠ¤íŠ¸ OSì˜ 9092 í¬íŠ¸ë¥¼ ì‚¬ìš© (í¬íŠ¸ í¬ì›Œë”©)
  const FIXED_KAFKA_PORT = 9092;
  const host = 'localhost';
  const kafkaNodeId = 1;

  // KafkaContainerëŠ” GenericContainerë¥¼ ìƒì†ë°›ìœ¼ë¯€ë¡œ
  // withExposedPortsë¡œ í¬íŠ¸ ë°”ì¸ë”©(í¬íŠ¸ í¬ì›Œë”©) ì„¤ì • ê°€ëŠ¥
  // í¬íŠ¸ ë°”ì¸ë”© í˜•ì‹: { container: 9092, host: 9092 }
  const kafkaContainer = new GenericContainer('confluentinc/cp-kafka:7.5.0')
    .withNetwork(network)
    .withNetworkAliases(KAFKA_HOST_NAME)
    .withExposedPorts({
      container: 9092,
      host: FIXED_KAFKA_PORT,
    }); // í¬íŠ¸ í¬ì›Œë”©: í˜¸ìŠ¤íŠ¸ 9092 -> ì»¨í…Œì´ë„ˆ 9092

  // Kafka ì‹œì‘
  kafka = await kafkaContainer
    .withEnvironment({
      KAFKA_NODE_ID: kafkaNodeId.toString(),
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP:
        'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT',
      KAFKA_ADVERTISED_LISTENERS: `PLAINTEXT://${KAFKA_HOST_NAME}:29092,PLAINTEXT_HOST://localhost:9092`,
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: '1',
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: '0',
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: '1',
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: '1',
      KAFKA_JMX_PORT: '9101',
      KAFKA_JMX_HOSTNAME: 'localhost',
      KAFKA_PROCESS_ROLES: 'broker,controller',
      KAFKA_CONTROLLER_QUORUM_VOTERS: `${kafkaNodeId}@${KAFKA_HOST_NAME}:29093`,
      KAFKA_LISTENERS: `PLAINTEXT://${KAFKA_HOST_NAME}:29092,CONTROLLER://${KAFKA_HOST_NAME}:29093,PLAINTEXT_HOST://0.0.0.0:9092`,
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT',
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER',
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs',
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk',
    })
    .start();

  kafkaUrl = `PLAINTEXT://${host}:${FIXED_KAFKA_PORT}`;
  console.log('âœ… Kafka started', `url: ${kafkaUrl}`);
  console.log(
    `   Internal (Docker network): ${kafkaInternalBootstrapServer}, External (Host OS): ${host}:${FIXED_KAFKA_PORT}`,
  );

  const kafkaHost = kafka?.getHost();
  const kafkaPort = kafka?.getMappedPort(9092);

  console.log(
    `   Port forwarding: ${kafkaHost}:${kafkaPort.toString()} -> ${host}:${FIXED_KAFKA_PORT}`,
  );

  // Kafkaê°€ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
  console.log('â³ Waiting for Kafka to be fully ready...');
  await waitForKafkaReady(kafkaHost, kafkaPort, 60);
  console.log('âœ… Kafka is ready to accept connections');
}

async function waitForKafkaReady(host: string, port: number, maxRetries = 30) {
  console.log(
    'ğŸ”„ Waiting for Kafka to be ready...',
    `host: ${host}, port: ${port}`,
  );
  const net = await import('net');
  for (let i = 0; i < maxRetries; i++) {
    const isReady = await new Promise<boolean>((resolve) => {
      const socket = new net.Socket();
      const timeout = setTimeout(() => {
        socket.destroy();
        resolve(false);
      }, 1000);

      socket.on('connect', () => {
        clearTimeout(timeout);
        socket.destroy();
        resolve(true);
      });

      socket.on('error', () => {
        clearTimeout(timeout);
        resolve(false);
      });

      socket.connect(port, host);
    });

    if (isReady) {
      // ì—°ê²°ì´ ì„±ê³µí–ˆì–´ë„ Kafkaê°€ ì™„ì „íˆ ì¤€ë¹„ë˜ë ¤ë©´ ì¶”ê°€ ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
      await new Promise((resolve) => setTimeout(resolve, 2000));
      return;
    }

    await new Promise((resolve) => setTimeout(resolve, 1000));
  }
  throw new Error(
    `Kafka at ${host}:${port} did not become ready within ${maxRetries} seconds`,
  );
}

async function startDebeziumConnect(
  network: StartedNetwork,
  options: {
    bootstrapServer: string;
    postgresHost: string;
    postgresPort: string;
    postgresUsername: string;
    postgresPassword: string;
    postgresDatabase: string;
  },
) {
  const {
    bootstrapServer,
    postgresHost,
    postgresPort,
    postgresUsername,
    postgresPassword,
    postgresDatabase,
  } = options;
  // Debezium Kafka Connect
  console.log('ğŸ”„ Starting Debezium Connect...');
  connect = await new GenericContainer('debezium/connect:2.6')
    .withNetwork(network)
    .withNetworkAliases(DEBEZIUM_HOST_NAME)
    .withEnvironment({
      BOOTSTRAP_SERVERS: bootstrapServer,
      GROUP_ID: '1',
      CONFIG_STORAGE_TOPIC: 'connect-configs',
      OFFSET_STORAGE_TOPIC: 'connect-offsets',
      STATUS_STORAGE_TOPIC: 'connect-status',
    })
    .withExposedPorts(8083)
    .withWaitStrategy(Wait.forHttp('/connectors', 8083).forStatusCode(200))
    .start();

  const connectUrl = `http://${connect?.getHost()}:${connect?.getMappedPort(8083).toString()}`;
  console.log('âœ… Debezium Connect started', `url: ${connectUrl}`);

  console.log('ğŸ”„ Creating Debezium Connect connector...');
  const response = await fetch(`${connectUrl}/connectors`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: DEBEZIUM_POSTGRES_CONNECTOR_CONFIG_FOR_EXTERNAL_DB(
      postgresHost,
      postgresPort,
      postgresDatabase,
      postgresUsername,
      postgresPassword,
    ),
  });
  if (!response.ok) {
    const errorBody = await response.text();
    throw new Error(
      `Connector creation failed: ${response.status} ${response.statusText} - ${errorBody}`,
    );
  }
  console.log('âœ… Debezium Connect connector created', await response.json());
}

async function startElasticsearch(network: StartedNetwork) {
  // Elasticsearch
  console.log('ğŸ”„ Starting Elasticsearch...');
  elastic = await new ElasticsearchContainer(
    'docker.elastic.co/elasticsearch/elasticsearch:8.14.0',
  )
    .withNetwork(network)
    .withNetworkAliases(ELASTICSEARCH_HOST_NAME)
    .withEnvironment({
      'discovery.type': 'single-node',
      'xpack.security.enabled': 'false',
      'xpack.security.http.ssl.enabled': 'false',
      ELASTIC_USERNAME,
      ELASTIC_PASSWORD,
    })
    .withExposedPorts({
      container: 9200,
      host: 9200,
    })
    .start();
  elasticUrl = `http://${elastic?.getHost()}:${elastic?.getMappedPort(9200).toString()}`;
  internalElasticsearchUrl = `http://${ELASTICSEARCH_HOST_NAME}:9200`;
  console.log('âœ… Elasticsearch started', `url: ${elasticUrl}`);
}

async function startKibana(network: StartedNetwork, elasticUrl: string) {
  console.log('ğŸ”„ Starting Kibana...');
  const kibanaElasticsearchHosts = JSON.stringify([elasticUrl]);
  kibana = await new GenericContainer('docker.elastic.co/kibana/kibana:8.14.0')
    .withNetwork(network)
    .withNetworkAliases(KIBANA_HOST_NAME)
    .withEnvironment({
      ELASTICSEARCH_HOSTS: kibanaElasticsearchHosts,
    })
    .withExposedPorts(5601)
    .start();
  kibanaUrl = `http://${kibana?.getHost()}:${kibana?.getMappedPort(5601).toString()}`;
  console.log('âœ… Kibana started', `url: ${kibanaUrl}`);
}

async function startKafkaUi(
  network: StartedNetwork,
  options: { kafkaBootstrapServer: string },
) {
  const { kafkaBootstrapServer } = options;
  console.log('ğŸ”„ Starting Kafka UI...');
  kafkaUi = await new GenericContainer('provectuslabs/kafka-ui:latest')
    .withNetwork(network)
    .withHostname(KAFKA_UI_HOST_NAME)
    .withEnvironment({
      KAFKA_CLUSTERS_0_NAME: 'local',
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafkaBootstrapServer,
      SERVER_PORT: '8080',
    })
    .withExposedPorts(8080)
    // .withWaitStrategy(Wait.forHttp('/actuator/health', 8080).forStatusCode(200))
    .start();
  kafkaUiUrl = `http://${kafkaUi?.getHost()}:${kafkaUi
    ?.getMappedPort(8080)
    .toString()}`;
  console.log('âœ… Kafka UI started', `url: ${kafkaUiUrl}`);
}

async function startKSQLDB(
  network: StartedNetwork,
  bootstrapKafkaServer: string,
) {
  console.log('ğŸ”„ Starting KSQLDB...');
  // NOTE:
  // - 0.36.0 íƒœê·¸ëŠ” Docker Hubì— ì—†ìŒ â†’ manifest unknown 404 ì—ëŸ¬ ë°œìƒ
  // - í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ìœ ì§€ë³´ìˆ˜ë˜ëŠ” latest íƒœê·¸ë¥¼ ì‚¬ìš©
  ksqlDb = await new GenericContainer('confluentinc/ksqldb-server:latest')
    .withNetwork(network)
    .withNetworkAliases(KSQLDB_HOST_NAME)
    .withEnvironment({
      KSQL_LISTENERS: 'http://0.0.0.0:8088',
      KSQL_BOOTSTRAP_SERVERS: bootstrapKafkaServer,
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true',
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true',
      KSQL_CONFIG_DIR: '/etc/ksqldb',
      KSQL_STREAMS_AUTO_OFFSET_RESET: 'earliest',
    })
    .withExposedPorts(8088)
    .withWaitStrategy(
      Wait.forHttp('/info', 8088).forStatusCode(200).withStartupTimeout(60000),
    )
    .start();
  const ksqlDbUrl = `http://${ksqlDb?.getHost()}:${ksqlDb
    ?.getMappedPort(8088)
    .toString()}`;
  console.log('âœ… KSQLDB started', `url: ${ksqlDbUrl}`);

  // KSQLDBê°€ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
  console.log('â³ Waiting for KSQLDB to be fully ready...');
  await waitForKSQLDBReady(ksqlDbUrl, 60);
  console.log('âœ… KSQLDB is ready to accept queries');

  // KSQL ì¿¼ë¦¬ ì‹¤í–‰
  await setupKSQLQueries(ksqlDbUrl);
}

async function waitForKSQLDBReady(ksqlDbUrl: string, maxRetries = 60) {
  console.log('ğŸ”„ Waiting for KSQLDB to be ready...', `url: ${ksqlDbUrl}`);
  for (let i = 0; i < maxRetries; i++) {
    try {
      const response = await fetch(`${ksqlDbUrl}/info`);
      if (response.ok) {
        const data = (await response.json()) as {
          KsqlServerInfo?: { version?: string };
        };
        if (data.KsqlServerInfo?.version) {
          // KSQLDBê°€ ì™„ì „íˆ ì¤€ë¹„ë˜ë ¤ë©´ ì¶”ê°€ ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
          await new Promise((resolve) => setTimeout(resolve, 3000));
          return;
        }
      }
    } catch {
      // ì—°ê²° ì‹¤íŒ¨ëŠ” ì •ìƒ (ì•„ì§ ì‹œì‘ ì¤‘)
    }
    await new Promise((resolve) => setTimeout(resolve, 1000));
  }
  throw new Error(
    `KSQLDB at ${ksqlDbUrl} did not become ready within ${maxRetries} seconds`,
  );
}

async function setupKSQLQueries(ksqlDbUrl: string) {
  console.log('ğŸ”„ Setting up KSQL queries...');

  // KSQL REST APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
  const executeKSQL = async (
    ksql: string,
    streamsProperties: Record<string, string> = {},
    retries = 3,
  ) => {
    for (let attempt = 1; attempt <= retries; attempt++) {
      try {
        const response = await fetch(`${ksqlDbUrl}/ksql`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/vnd.ksql.v1+json',
          },
          body: JSON.stringify({
            ksql,
            streamsProperties,
          }),
        });

        if (!response.ok) {
          const errorText = await response.text();
          // ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°ëŠ” ë¬´ì‹œ (ì—ëŸ¬ ë©”ì‹œì§€ì— "already exists" í¬í•¨)
          if (
            errorText.includes('already exists') ||
            errorText.includes('already registered')
          ) {
            console.log(
              `   â„¹ï¸  ${ksql.split(' ')[0]} already exists, skipping...`,
            );
            return;
          }
          throw new Error(
            `KSQL query failed: ${response.status} ${response.statusText} - ${errorText}`,
          );
        }

        const result = (await response.json()) as Array<{
          errorMessage?: string;
        }>;
        if (result[0]?.errorMessage) {
          // ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°ëŠ” ë¬´ì‹œ
          const errorMsg = result[0].errorMessage;
          if (
            errorMsg.includes('already exists') ||
            errorMsg.includes('already registered')
          ) {
            console.log(
              `   â„¹ï¸  ${ksql.split(' ')[0]} already exists, skipping...`,
            );
            return;
          }
          throw new Error(`KSQL query error: ${errorMsg}`);
        }
        return result;
      } catch (error) {
        if (attempt < retries) {
          const waitTime = attempt * 1000; // 1ì´ˆ, 2ì´ˆ, 3ì´ˆ ëŒ€ê¸°
          console.log(
            `   âš ï¸  Attempt ${attempt} failed, retrying in ${waitTime}ms...`,
          );
          await new Promise((resolve) => setTimeout(resolve, waitTime));
          continue;
        }
        throw error;
      }
    }
    throw new Error('All retry attempts failed');
  };

  // ê¸°ì¡´ ìŠ¤íŠ¸ë¦¼/í…Œì´ë¸” ì‚­ì œ (earliest offsetì„ ì ìš©í•˜ê¸° ìœ„í•´)
  const dropIfExists = async (name: string, type: 'STREAM' | 'TABLE') => {
    try {
      await executeKSQL(`DROP ${type} IF EXISTS ${name} DELETE TOPIC;`);
    } catch {
      // ì‚­ì œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    }
  };

  try {
    // ì „ì—­ ì„¤ì •: earliest offsetìœ¼ë¡œ ì„¤ì •
    // console.log('   âš™ï¸  Setting global auto.offset.reset to earliest...');
    // await executeKSQL(`SET 'auto.offset.reset'='earliest';`);
    // console.log('   âœ… Global offset reset configured');

    // ê¸°ì¡´ ê°ì²´ë“¤ì„ ë¨¼ì € ì‚­ì œ (earliest offsetì„ ì ìš©í•˜ê¸° ìœ„í•´)
    // ê¸°ì¡´ consumer group offsetì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ earliestê°€ ì ìš©ë˜ì§€ ì•ŠìŒ
    console.log('   ğŸ—‘ï¸  Cleaning up existing KSQL objects to reset offsets...');
    await dropIfExists('cafe_info_with_region_mv', 'TABLE');
    await dropIfExists('cafe_info_table', 'TABLE');
    await dropIfExists('region_category_table', 'TABLE');
    await dropIfExists('cafe_info_extracted', 'STREAM');
    await dropIfExists('region_category_stream', 'STREAM');
    await dropIfExists('cafe_info_stream', 'STREAM');
    console.log('   âœ… Cleanup completed - offsets will be reset to earliest');
    await new Promise((resolve) => setTimeout(resolve, 2000)); // ì‚­ì œ ì™„ë£Œ ëŒ€ê¸°

    // 1. CafeInfo í† í”½ì„ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ìƒì„±
    // Debezium ë©”ì‹œì§€ í˜•ì‹: before, after, op í•„ë“œë¥¼ í¬í•¨
    console.log('   ğŸ“ Creating stream for dbserver.public.CafeInfo...');
    const resultStreamCafeInfo = await executeKSQL(
      `CREATE STREAM IF NOT EXISTS stream_cafe_info (
        before STRUCT<
          id BIGINT,
          "createdAt" BIGINT,
          "isDisable" BOOLEAN,
          name VARCHAR,
          code VARCHAR,
          "regionCategoryId" BIGINT,
          address VARCHAR,
          directions VARCHAR,
          "businessNumber" VARCHAR,
          "ceoName" VARCHAR
        >,
        after STRUCT<
          id BIGINT,
          "createdAt" BIGINT,
          "isDisable" BOOLEAN,
          name VARCHAR,
          code VARCHAR,
          "regionCategoryId" BIGINT,
          address VARCHAR,
          directions VARCHAR,
          "businessNumber" VARCHAR,
          "ceoName" VARCHAR
        >,
        op VARCHAR,
        ts_ms BIGINT
      ) WITH (
        KAFKA_TOPIC='dbserver.public.CafeInfo',
        VALUE_FORMAT='JSON'
      );
    `,
      { 'ksql.streams.auto.offset.reset': 'earliest' },
    );
    console.log('âœ… CafeInfo stream created', resultStreamCafeInfo);
    await new Promise((resolve) => setTimeout(resolve, 1000)); // ì¿¼ë¦¬ ì‚¬ì´ ëŒ€ê¸°

    // ì‹¤ì œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ìŠ¤íŠ¸ë¦¼ ìƒì„±
    // c(create), u(update): after í•„ë“œì—ì„œ ë°ì´í„° ì¶”ì¶œ
    // d(delete): before í•„ë“œì—ì„œ ë°ì´í„° ì¶”ì¶œ
    // r(read/snapshot): after í•„ë“œì—ì„œ ë°ì´í„° ì¶”ì¶œ
    await executeKSQL(
      `
      CREATE STREAM IF NOT EXISTS stream_cafe_info_extracted 
      AS
      SELECT 
        COALESCE(after->id, before->id) AS id,
        COALESCE(after->"createdAt", before->"createdAt") AS "createdAt",
        COALESCE(after->"isDisable", before->"isDisable") AS "isDisable",
        COALESCE(after->name, before->name) AS name,
        COALESCE(after->code, before->code) AS code,
        COALESCE(after->"regionCategoryId", before->"regionCategoryId") AS "regionCategoryId",
        COALESCE(after->address, before->address) AS address,
        COALESCE(after->directions, before->directions) AS directions,
        COALESCE(after->"businessNumber", before->"businessNumber") AS "businessNumber",
        COALESCE(after->"ceoName", before->"ceoName") AS "ceoName",
        op,
        before,
        after,
        ts_ms
      FROM stream_cafe_info
      WHERE (after IS NOT NULL OR before IS NOT NULL)
      PARTITION BY COALESCE(after->id, before->id)
      EMIT CHANGES;
    `,
      { 'ksql.streams.auto.offset.reset': 'earliest' },
    );

    // CafeInfo í…Œì´ë¸” ìƒì„± (ìµœì‹  ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” KSQL TABLE)
    await executeKSQL(
      `
      CREATE TABLE IF NOT EXISTS tbl_cafe_info 
      WITH (KEY_FORMAT='JSON')
      AS
      SELECT 
        id,
        LATEST_BY_OFFSET("createdAt") AS "createdAt",
        LATEST_BY_OFFSET("isDisable") AS "isDisable",
        LATEST_BY_OFFSET(name) AS name,
        LATEST_BY_OFFSET(code) AS code,
        LATEST_BY_OFFSET("regionCategoryId") AS "regionCategoryId",
        LATEST_BY_OFFSET(address) AS address,
        LATEST_BY_OFFSET(directions) AS directions,
        LATEST_BY_OFFSET("businessNumber") AS "businessNumber",
        LATEST_BY_OFFSET("ceoName") AS "ceoName"
      FROM stream_cafe_info_extracted
      GROUP BY id
      EMIT CHANGES;
    `,
      { 'ksql.streams.auto.offset.reset': 'earliest' },
    );

    // 2. RegionCategory í† í”½ì„ í…Œì´ë¸”ë¡œ ìƒì„± (JOINì„ ìœ„í•´ í…Œì´ë¸” ì‚¬ìš©)
    console.log('   ğŸ“ Creating table for dbserver.public.RegionCategory...');
    const resultStreamRegionCategory = await executeKSQL(
      `CREATE STREAM IF NOT EXISTS stream_region_category (
        before STRUCT<
          id BIGINT,
          "createdAt" BIGINT,
          name VARCHAR,
          "isDisable" BOOLEAN,
          "govermentType" VARCHAR
        >,
        after STRUCT<
          id BIGINT,
          "createdAt" BIGINT,
          name VARCHAR,
          "isDisable" BOOLEAN,
          "govermentType" VARCHAR
        >,
        op VARCHAR,
        ts_ms BIGINT
      ) WITH (
        KAFKA_TOPIC='dbserver.public.RegionCategory',
        VALUE_FORMAT='JSON'
      );
    `,
      { 'ksql.streams.auto.offset.reset': 'earliest' },
    );
    console.log('âœ… RegionCategory stream created', resultStreamRegionCategory);
    await new Promise((resolve) => setTimeout(resolve, 1000)); // ì¿¼ë¦¬ ì‚¬ì´ ëŒ€ê¸°

    // RegionCategory í…Œì´ë¸” ìƒì„± (after í•„ë“œì—ì„œ ë°ì´í„° ì¶”ì¶œ)
    await executeKSQL(
      `
      CREATE TABLE IF NOT EXISTS tbl_region_category 
      WITH (KEY_FORMAT='JSON')
      AS
      SELECT 
        after->id AS id,
        LATEST_BY_OFFSET(after->"createdAt") AS "createdAt",
        LATEST_BY_OFFSET(after->name) AS name,
        LATEST_BY_OFFSET(after->"isDisable") AS "isDisable",
        LATEST_BY_OFFSET(after->"govermentType") AS "govermentType"
      FROM stream_region_category
      WHERE after IS NOT NULL
      GROUP BY after->id
      EMIT CHANGES;
    `,
      { 'ksql.streams.auto.offset.reset': 'earliest' },
    );

    // 3. TABLE*TABLE CTASë¡œ ë°˜ì •ê·œí™”ëœ CafeInfo+RegionCategory ë·° ìƒì„±
    // - ê¸°ì¤€: cafe_info_table (ìµœì‹  CafeInfo ìƒíƒœ)
    // - JOIN: region_category_table (ìµœì‹  RegionCategory ìƒíƒœ)
    // ì´ CTAS TABLEì˜ changelog í† í”½ì€ CafeInfo/RegionCategory ì–´ëŠ ìª½ì´ ë°”ë€Œì–´ë„ ê°±ì‹  ì´ë²¤íŠ¸ë¥¼ ë°œí–‰í•¨
    console.log(
      '   ğŸ“ Creating denormalized TABLE: mv_cafe_info_with_region (TABLE*TABLE)...',
    );
    await executeKSQL(
      `
      CREATE TABLE IF NOT EXISTS mv_cafe_info_with_region AS
      SELECT 
        ci.id                   AS "id",
        ci."createdAt"          AS "createdAt",
        ci."isDisable"          AS "isDisable",
        ci.name                 AS "name",
        ci.code                 AS "code",
        ci."regionCategoryId"   AS "regionCategoryId",
        ci.address              AS "address",
        ci.directions           AS "directions",
        ci."businessNumber"     AS "businessNumber",
        ci."ceoName"            AS "ceoName",
        STRUCT(
          "id" := rc.id,
          "createdAt" := rc."createdAt",
          "name" := rc.name,
          "isDisable" := rc."isDisable",
          "govermentType" := rc."govermentType"
        ) AS "RegionCategory"
      FROM tbl_cafe_info ci
      INNER JOIN tbl_region_category rc
        ON ci."regionCategoryId" = rc.id
      EMIT CHANGES;
    `,
      { 'ksql.streams.auto.offset.reset': 'earliest' },
    );

    console.log('âœ… KSQL queries setup completed');
    console.log('   ğŸ“Š Created table: mv_cafe_info_with_region');
    console.log('   ğŸ“Š Output topic (changelog): CAFE_INFO_WITH_REGION_MV');
  } catch (error) {
    console.error('âŒ Failed to setup KSQL queries:', error);
    throw error;
  }
}

async function stopPostgres() {
  console.log('ğŸ”„ Stopping Postgres...');
  await postgres?.stop();
  console.log('âœ… Postgres stopped');
}

async function stopKafka() {
  console.log('ğŸ”„ Stopping Kafka...');
  await kafka?.stop({
    removeVolumes: true,
  });
  console.log('âœ… Kafka stopped');
}

async function stopDebeziumConnect() {
  console.log('ğŸ”„ Stopping Debezium Connect...');
  await connect?.stop();
  console.log('âœ… Debezium Connect stopped');
}

async function stopElasticsearch() {
  console.log('ğŸ”„ Stopping Elasticsearch...');
  await elastic?.stop({
    removeVolumes: true,
  });
  console.log('âœ… Elasticsearch stopped');
}

async function stopKibana() {
  console.log('ğŸ”„ Stopping Kibana...');
  await kibana?.stop({
    removeVolumes: true,
  });
  console.log('âœ… Kibana stopped');
}

async function stopKafkaUi() {
  console.log('ğŸ”„ Stopping Kafka UI...');
  await kafkaUi?.stop();
  console.log('âœ… Kafka UI stopped');
}

async function stopKSQLDB() {
  console.log('ğŸ”„ Stopping KSQLDB...');
  await ksqlDb?.stop({
    removeVolumes: true,
  });
  console.log('âœ… KSQLDB stopped');
}

let isStartCleanUp = false;
async function cleanUp(code: number = 0) {
  if (isStartCleanUp) {
    console.log('ğŸ§¹ Clean up already started. Skipping... code: ', code);
    return;
  }
  isStartCleanUp = true;
  console.log('ğŸ§¹ Cleaning up... code: ', code);
  await stopPostgres();
  await stopKafka();
  await stopDebeziumConnect();
  await stopKibana();
  await stopElasticsearch();
  await stopKafkaUi();
  await stopKSQLDB();
  await removeNetwork();
  stopNestJS();
  console.log('âœ… Clean up completed');
  process.exit(code || 0);
}

function startNestJS({
  shouldStartAppWithWatch,
  regionCategoriesBaseUrl,
  elasticsearchUrl,
  kafkaUrl,
}: {
  shouldStartAppWithWatch: boolean;
  regionCategoriesBaseUrl: string;
  elasticsearchUrl?: string;
  kafkaUrl: string;
}) {
  // NestJS ì•±ì„ spawnìœ¼ë¡œ ì‹œì‘ (í™˜ê²½ë³€ìˆ˜ëŠ” ì´ë¯¸ process.envì— ì„¤ì •ë¨)
  const isWindows = process.platform === 'win32';
  const nestCliPath = isWindows
    ? path.resolve(process.cwd(), './node_modules/.bin/nest.cmd')
    : path.resolve(process.cwd(), './node_modules/.bin/nest');
  nestProcess = spawn(
    nestCliPath,
    ['start', shouldStartAppWithWatch ? '--watch' : ''],
    {
      env: {
        ...process.env,
        REGION_CATEGORIES_BASE_URL: regionCategoriesBaseUrl,
        ELASTICSEARCH_HOSTS: elasticsearchUrl,
        ELASTICSEARCH_USERNAME: ELASTIC_USERNAME,
        ELASTICSEARCH_PASSWORD: ELASTIC_PASSWORD,
        // PLAINTEXT:// í”„ë¦¬í”½ìŠ¤ ì œê±° (ì˜ˆ: PLAINTEXT://localhost:9092 -> localhost:9092)
        KAFKA_BROKERS: kafkaUrl.replace(/^PLAINTEXT:\/\//, ''),
      },
      shell: isWindows,
      stdio: 'inherit', // ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ì˜ stdioë¥¼ ìƒì†
      cwd: path.resolve(process.cwd()),
    },
  );
  nestProcess.on('exit', (code) => {
    console.log('âœ… NestJS app exited with code: ', code);
    void cleanUp(code || 0);
  });

  return nestProcess;
}

function stopNestJS() {
  console.log('ğŸ”„ Stopping NestJS...');
  nestProcess?.kill();
  console.log('âœ… NestJS stopped');
  nestProcess = undefined;
}

async function removeNetwork() {
  console.log('ğŸ”„ Removing network...');
  await network?.stop();
  console.log('âœ… Network removed');
  network = undefined;
}

async function bootstrap(args: string[]) {
  network = await new Network({
    nextUuid: () => 'mecipe-network-test',
  }).start();

  let commandParameters: string[] = [];
  try {
    commandParameters = getCommandParameters('--exclude', args).flat();
  } catch {
    console.error('ëª¨ë‘ ì‹¤í–‰í•©ë‹ˆë‹¤. --exclude ì˜µì…˜ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }
  const excludePostgres = commandParameters.includes('postgres');
  if (excludePostgres) console.log('ğŸ”„ Postgresë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  const excludeKafka = commandParameters.includes('kafka');
  if (excludeKafka) console.log('ğŸ”„ Kafkaë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  const excludeDebeziumConnect = commandParameters.includes('debezium-connect');
  if (excludeDebeziumConnect)
    console.log('ğŸ”„ Debezium Connectë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  const excludeKafkaUi = commandParameters.includes('kafka-ui');
  if (excludeKafkaUi) console.log('ğŸ”„ Kafka UIë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  const excludeKibana = commandParameters.includes('kibana');
  if (excludeKibana) console.log('ğŸ”„ Kibanaë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  const excludeElasticsearch = commandParameters.includes('elasticsearch');
  if (excludeElasticsearch)
    console.log('ğŸ”„ Elasticsearchë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  const excludeKSQLDB = commandParameters.includes('ksqldb');
  if (excludeKSQLDB) console.log('ğŸ”„ KSQLDBë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  if (!excludePostgres) {
    await startPostgres(network);
  }
  if (!excludeKafka) {
    await startKafka(network);
  }
  if (!excludeDebeziumConnect) {
    let _kafkaInternalBootstrapServer = kafkaInternalBootstrapServer;
    if (!_kafkaInternalBootstrapServer) {
      const kafkaParameter = getCommandParameters('--kafka', args)[0];
      _kafkaInternalBootstrapServer = kafkaParameter[0];
    }
    let postgresHost: string | undefined;
    let postgresPort: string | undefined;
    let postgresUsername: string | undefined;
    let postgresPassword: string | undefined;
    let postgresDatabase: string | undefined;
    if (postgres) {
      postgresHost = 'host.docker.internal';
      postgresPort = postgres?.getMappedPort(5432).toString();
      postgresUsername = postgres?.getUsername();
      postgresPassword = postgres?.getPassword();
      postgresDatabase = postgres?.getDatabase();
    } else {
      const postgresParameter = getCommandParameters('--postgres', args)[0];
      postgresHost = postgresParameter[0];
      postgresPort = postgresParameter[1];
      postgresUsername = postgresParameter[2];
      postgresPassword = postgresParameter[3];
      postgresDatabase = postgresParameter[4];
    }
    await startDebeziumConnect(network, {
      bootstrapServer: _kafkaInternalBootstrapServer,
      postgresHost,
      postgresPort: postgresPort.toString(),
      postgresUsername: postgresUsername.toString(),
      postgresPassword: postgresPassword.toString(),
      postgresDatabase: postgresDatabase.toString(),
    });
  }
  if (!excludeElasticsearch) {
    await startElasticsearch(network);
  }
  if (!excludeKafkaUi) {
    let bootstrapServer = kafkaInternalBootstrapServer;
    if (!bootstrapServer) {
      const kafkaParameter = getCommandParameters('--kafka', args)[0];
      if (!kafkaParameter?.[0]) {
        throw new Error(
          'âŒ Kafka bootstrap server is required for Kafka UI (--kafka-url or start Kafka container)',
        );
      }
      bootstrapServer = kafkaParameter[0].replace(/^PLAINTEXT:\/\//, '');
    }
    if (!bootstrapServer) {
      throw new Error('âŒ Kafka bootstrap server is required for Kafka UI');
    }
    await startKafkaUi(network, { kafkaBootstrapServer: bootstrapServer });
  }
  if (!excludeKibana) {
    const _elasticsearchUrl = internalElasticsearchUrl;
    if (!_elasticsearchUrl) {
      throw new Error('âŒ Elasticsearch URL is required for Kibana');
    }
    await startKibana(network, _elasticsearchUrl);
  }
  if (!excludeKSQLDB) {
    let bootstrapServer = kafkaInternalBootstrapServer;
    if (!bootstrapServer) {
      const kafkaParameter = getCommandParameters('--kafka', args)[0];
      if (!kafkaParameter?.[0]) {
        throw new Error(
          'âŒ Kafka bootstrap server is required for KSQLDB (--kafka-url or start Kafka container)',
        );
      }
      bootstrapServer = kafkaParameter[0].replace(/^PLAINTEXT:\/\//, '');
    }
    if (!bootstrapServer) {
      throw new Error('âŒ Kafka bootstrap server is required for KSQLDB');
    }
    await startKSQLDB(network, bootstrapServer);
  }
  const isStartApp = args.includes('--start-app');
  const isStartAppWithWatch = args.includes('--watch');
  const regionCategoriesBaseUrl = getCommandParameters(
    '--region-url',
    args,
  )[0][0];
  if (!regionCategoriesBaseUrl) {
    throw new Error('âŒ Region categories base URL is required');
  }
  let _elasticsearchUrl: string | undefined = elasticUrl;
  if (!_elasticsearchUrl) {
    const getElasticsearchUrl = getCommandParameters('--es-url', args)[0];
    _elasticsearchUrl = getElasticsearchUrl[0];
  }
  let _kafkaUrl: string | undefined = kafkaUrl;
  if (!_kafkaUrl) {
    const getKafkaUrl = getCommandParameters('--kafka-url', args)[0];
    _kafkaUrl = getKafkaUrl[0];
  }

  if (isStartApp) {
    startNestJS({
      shouldStartAppWithWatch: isStartAppWithWatch,
      regionCategoriesBaseUrl,
      elasticsearchUrl: _elasticsearchUrl,
      kafkaUrl: _kafkaUrl,
    });
  }
}

if (require.main === module) {
  bootstrap(process.argv.slice(2)).catch(async (error) => {
    console.error('âŒ Total infrastructure setup failed:', error);
    await cleanUp(1);
  });
  process.on('SIGINT', () => {
    void cleanUp(0);
  });
  process.on('SIGTERM', () => {
    void cleanUp(0);
  });
}
